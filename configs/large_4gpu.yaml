# CelebGen Large Model - 4 GPU Configuration (JiT-style)
# Reference: https://github.com/LTH14/JiT
# Usage: python scripts/train.py --config configs/large_4gpu.yaml

# Experiment
exp_name: celeba_large_4gpu
seed: 42

# Data
data_root: ./data
image_size: 64
batch_size: 64
num_workers: 4

# Model
model_size: large
patch_size: 4

# Training (JiT-style)
epochs: 200
lr: 5e-5  # JiT uses 5e-5 as base LR
weight_decay: 0.0
grad_clip: 1.0

# Loss parameters (JiT-style logit-normal sampling)
P_mean: -0.8
P_std: 0.8
noise_scale: 1.0
t_eps: 1e-5

# Scheduler
scheduler: cosine
warmup_epochs: 10
warmup_start_factor: 0.1

# Sampling
num_sampling_steps: 100
num_samples: 16
sampling_method: euler  # euler or heun

# FID
fid_num_samples: 2000
fid_batch_size: 64
fid_interval: 10  # Compute FID every N epochs (0=disable)

# Multi-GPU
num_gpus: 4
dist_backend: nccl

# Logging
log_interval: 50
save_interval: 10

